---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/cover2.jpg">
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        Research Scientist<br>
        FAIR, Meta AI<br>
        abhshkdz at meta dot com
    </div>
</div>
<hr>

<a name="/news"></a>

# News

- [Feb 22] Runner-up for the [2020 AAAI/ACM SIGAI Doctoral Dissertation Award][aaai-dissertation-award].
- [Mar 21] Awarded the [Georgia Tech Sigma Xi Best PhD Thesis Award][sigma-xi-thesis-award].
- [Mar 21] Awarded the [Georgia Tech College of Computing Dissertation Award][coc-dissertation-award].
- [Nov 20] The [Open Catalyst Project][ocp] was covered by [Fortune][ocp-fortune], [Engadget][ocp-engadget], [CNBC][ocp-cnbc], [VentureBeat][ocp-venturebeat].
- [Nov 20] Organizing the 4th [Visually-Grounded Interaction & Language Workshop at NAACL][vigil20].
- [July 20] Presenting [Probing Emergent Semantics in Predictive Agents](#/qa-probing) at ICML 2020 ([Video][qa-probing-icml20-talk]).
- [Mar 20] I completed my PhD! My thesis, "Building agents that can see, talk, and act", is [here][thesis-pdf].
- [Nov 19] Organizing the [Visual Question Answering and Dialog workshop at CVPR 2020][51].
- [Sep 19] Organizing the [Visually-Grounded Interaction & Language Workshop at NeurIPS][vigil19].
- [Jun 19] Presenting [Targeted Multi-Agent Communication](#/multi-agent-comm) as an oral at ICML 2019 ([Video][tarmac-icml-talk]).
- [Mar 19] Co-founded [Caliper][caliper]. Caliper helps recruiters evaluate practical AI skills.
- [Feb 19] My work was featured in this [wonderful article by Georgia Tech][ic-gt-article].
- [Jan 19] Awarded the [Facebook Graduate Fellowship][fb-fellow-page].
- [Jan 19] Awarded the Microsoft Research PhD Fellowship (declined).
- [Jan 19] Awarded the NVIDIA Graduate Fellowship (declined).
- [Jan 19] Organizing the [2nd Visual Dialog Challenge][visdial-challenge-2].
- [Oct 18] Presenting [Neural Modular Control for Embodied QA](#/eqa-modular) at CoRL 2018 ([Video][58]).
- [Sep 18] Presenting [results and analysis of the 1st Visual Dialog Challenge][57] at ECCV 2018.
- [Jul 18] Presenting a [tutorial on Connecting Language and Vision to Actions][49] at [ACL 2018][50].
- [Jun 18] Organizing the 1st [Visual Dialog Challenge][53].
- [Jun 18] Presenting [Embodied Question Answering](#/embodied-qa) as an oral at CVPR 2018 ([Video][54]).
- [Jun 18] Organizing the [VQA Challenge and Visual Dialog Workshop at CVPR 2018][51].
- [Mar 18] Speaking on [Embodied Question Answering][40] at [NVIDIA GTC][42] ([Video][52]).
- [Dec 17] Awarded the [Adobe Research Fellowship][39]. ([Department's news story][44])
- [Dec 17] Awarded the [Snap Inc. Research Fellowship][36]. ([Department's news story][43])
- [Oct 17] Presenting [Cooperative Visual Dialog Agents](#/visdial-rl) as an oral at ICCV 2017 ([Video][37]).
- [Jul 17] Presenting [Visual Dialog](//visualdialog.org) at the [VQA Challenge Workshop](http://visualqa.org/workshop.html), CVPR 2017 ([Video][41]).
- [Jul 17] Presenting our paper on [Visual Dialog](#/visdial) as a spotlight at CVPR 2017 ([Video][38]).

<div id="read-more-button">
    <a nohref>Read more</a>
</div>

<hr>

<a name="/bio"></a>

# Bio

I am a Research Scientist at Fundamental AI Research (FAIR) at Meta AI working on deep neural
networks and its applications in climate change. My current focus is on
electrocatalyst discovery for renewable energy storage as part of the [Open Catalyst Project][ocp].
Renewable energy sources (like solar, wind) are great but intermittent -- the sun
shines only during the day. During the evening, we fall back on fossil fuels.
To avoid this, we need to discover cheap, scalable ways of converting electricity
from renewable sources to storable forms, so that we can transfer it from times of
peak generation to peak demand.
AI can help accelerate the chemical simulations needed to make these discoveries.

Before this, I was a Computer Science PhD student at Georgia Tech, advised by [Dhruv Batra][2],
and working closely with [Devi Parikh][3], where I focused on developing
artificial agents that can [<i>see</i> (computer vision), <i>talk</i> (language modeling), and <i>act</i> (reinforcement learning)][thesis-pdf].

<div class="row" id="timeline-logos">
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//iitr.ac.in"><img src="/img/logos/iitr.jpg"></a>
        </div>
        <div class="logo-desc">
            IIT Roorkee<br>
            2011 - 2015
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//qbi.uq.edu.au"><img style="width:120px;" src="/img/logos/uq.png"></a>
        </div>
        <div class="logo-desc">
            Queensland Brain Institute<br>
            Summer 2015
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//vt.edu"><img src="/img/logos/vt.png"></a>
        </div>
        <div class="logo-desc">
            Virginia Tech<br>
            2015 - 2016
        </div>
    </div>
    <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//gatech.edu"><img src="/img/logos/gatech.png"></a>
        </div>
        <div class="logo-desc">
            Georgia Tech<br>
            2017 - 2020
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//research.fb.com/category/facebook-ai-research/"><img style="width:160px;" src="/img/logos/fair3.png"></a>
        </div>
        <div class="logo-desc">
            Facebook AI Research<br>
            S2017, W2018, S2018
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//deepmind.com"><img style="width:120px;" src="/img/logos/deepmind.png"></a>
        </div>
        <div class="logo-desc">
            DeepMind<br>
            Winter 2019
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//www.tesla.com/autopilotAI"><img style="width:120px;" src="/img/logos/tesla.jpg"></a>
        </div>
        <div class="logo-desc">
            Tesla Autopilot<br>
            Summer 2019
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//research.fb.com/category/facebook-ai-research/"><img style="width:160px;" src="/img/logos/fair3.png"></a>
        </div>
        <div class="logo-desc">
            Facebook AI Research<br>
            Present
        </div>
    </div>
</div>

During my PhD, I interned thrice at Facebook AI Research — Summer 2017 and Spring 2018
at Menlo Park, working with [Georgia Gkioxari][46],
[Devi Parikh][47] and [Dhruv Batra][48] on training embodied agents for navigation and
question-answering in simulated environments (see [embodiedqa.org][40]), and Summer
2018 at Montréal, working with [Mike Rabbat][55] and [Joelle Pineau][56] on
<a target="_blank" href="https://arxiv.org/abs/1810.11187">communication protocols in multi-agent reinforcement learning</a>.
In 2019, I interned at DeepMind in London working on grounded language learning
with [Felix Hill][felix-hill], [Laura Rimell][laura-rimell],
and [Stephen Clark][stephen-clark], and at Tesla Autopilot in Palo Alto working on
differentiable neural architecture search with [Andrej Karpathy][andrej-karpathy].

My PhD research was supported by fellowships from [Facebook][fb-fellow-page],
[Adobe][39], and [Snap][36].

Prior to joining grad school, I worked on neural coding in zebrafish tectum
as an intern under [Prof. Geoffrey Goodhill][4] and [Lilach Avitan][5]
at the [Goodhill Lab][6], Queensland Brain Institute.

I got my Bachelor's at [IIT Roorkee][31] in 2015.
During my undergrad, I took part in
Google Summer of Code ([2013][8] and [2014][9]),
won several competitions ([Yahoo! HackU!][10],
[Microsoft Code.Fun.Do.][11], Deloitte CCTC [2013][12] and [2014][13]),
and owe most of my programming/tinkering bent to [SDSLabs][16].

On the side, I built [aideadlin.es][34] (countdowns to a bunch of CV/NLP/ML/AI conference deadlines)
and [aipaygrad.es][aipaygrad.es] (statistics of industry job offers in AI),
[neural-vqa][19] and its extension [neural-vqa-attention][35],
[HackFlowy][20], [graf][21], [Erdős][17], [etc][22].
I also occasionally dabble in [generative art](/art).
I like [this map][conquerearth] tracking the places I've been to.
[Blog posts from a previous life](/archive).

---

<a name="/publications"></a>

# Publications

<a name="/adsorbml"></a>
<h2 class="pubt">AdsorbML: Accelerating Adsorption Energy Calculations with Machine Learning</h2>
<p class="pubd">
    <span class="authors">Janice Lan, Aini Palizhati, Muhammed Shuaibi, Brandon M. Wood, Brook Wander, Abhishek Das, Matt Uyttendaele, C. Lawrence Zitnick, Zachary W. Ulissi</span><br>
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2211.16486">Paper</a>
        <a target="_blank" href="https://github.com/open-catalyst-project/adsorbml">Code</a>
    </span>
</p>
<img src="/img/ocp/adsorbml.png">
<hr>

<a name="/pirlnav"></a>
<h2 class="pubt">PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav</h2>
<p class="pubd">
    <span class="authors">Ram Ramrakhya, Dhruv Batra, Erik Wijmans, Abhishek Das</span><br>
    <span class="conf">CVPR 2023, RRL workshop at ICLR 2023</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2301.07302">Paper</a>
        <a target="_blank" href="https://github.com/Ram81/pirlnav">Code</a>
        <a target="_blank" href="https://ram81.github.io/projects/pirlnav.html">Website</a>
    </span>
</p>
<img src="/img/habitat/pirlnav.png">
<hr>

<a name="/oc22"></a>
<h2 class="pubt">The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysis</h2>
<p class="pubd">
    <span class="authors">Richard Tran*, Janice Lan*, Muhammed Shuaibi*, Siddharth Goyal*, Brandon M. Wood*, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, Zachary Ulissi, C. Lawrence Zitnick</span><br>
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.08917">Paper</a>
        <a target="_blank" href="https://github.com/open-catalyst-project/ocp">Code</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/blob/main/DATASET.md#open-catalyst-2022-oc22">Dataset</a>
    </span>
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="https://ai.facebook.com/blog/accelerating-renewable-energy-with-a-new-data-set-for-green-hydrogen-fuel/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/fair2.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"... new dataset for green hydrogen fuel" by Janice, Siddharth, Ammar, Larry</span>
            </div>
        </a>
    </div>
</p>
<img src="/img/ocp/oc22.jpg">
<hr>

<a name="/gemnet-oc"></a>
<h2 class="pubt">GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets</h2>
<p class="pubd">
    <span class="authors">Johannes Gasteiger, Muhammed Shuaibi, Anuroop Sriram, Stephan Günnemann, Zachary Ulissi, C. Lawrence Zitnick, Abhishek Das</span><br>
    <span class="conf">TMLR 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2204.02782">Paper</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/gemnet_oc">Code</a>
    </span>
</p>
<img src="/img/ocp/gemnet_oc.jpg">
<hr>

<a name="/scn"></a>
<h2 class="pubt">Spherical Channels for Modeling Atomic Interactions</h2>
<p class="pubd">
    <span class="authors">C. Lawrence Zitnick, Abhishek Das, Adeesh Kolluru, Janice Lan, Muhammed Shuaibi, Anuroop Sriram, Zachary Ulissi, Brandon Wood</span><br>
    <span class="conf">NeurIPS 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a>
    </span>
</p>
<img src="/img/ocp/scn.jpg">
<hr>

<h2 class="pubt">Open Challenges in Developing Generalizable Large Scale Machine Learning Models for Catalyst Discovery</h2>
<p class="pubd">
    <span class="authors">Adeesh Kolluru*, Muhammed Shuaibi*, Aini Palizhati, Nima Shoghi, Abhishek Das, Brandon Wood, C. Lawrence Zitnick, John R. Kitchin, Zachary Ulissi</span><br>
    <span class="conf">ACS Catalysis (Perspective) 2022</span>
    <span class="links">
        <a target="_blank" href="https://pubs.acs.org/doi/10.1021/acscatal.2c02291">Paper</a>
    </span>
</p>
<img src="/img/ocp/perspective.jpg">
<hr>

<a name="/taag"></a>
<h2 class="pubt">Transfer learning using attentions across atomic systems with graph neural networks (TAAG)</h2>
<p class="pubd">
    <span class="authors">Adeesh Kolluru, Nima Shoghi, Muhammed Shuaibi, Siddharth Goyal, Abhishek Das, C. Lawrence Zitnick, Zachary Ulissi</span><br>
    <span class="conf">The Journal of Chemical Physics 2022</span>
    <span class="links">
        <a target="_blank" href="https://aip.scitation.org/doi/abs/10.1063/5.0088019?cookieSet=1">Paper</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/transfer_learning">Code</a>
    </span>
</p>
<hr>

<a name="/habitat-web"></a>
<h2 class="pubt">Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale</h2>
<p class="pubd">
    <span class="authors">Ram Ramrakhya, Eric Undersander, Dhruv Batra, Abhishek Das</span><br>
    <span class="conf">CVPR 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2204.03514">Paper</a>
        <a target="_blank" href="https://github.com/Ram81/habitat-web">Code</a>
        <a target="_blank" href="https://ram81.github.io/projects/habitat-web.html">Website</a>
        <a target="_blank" href="https://www.youtube.com/watch?v=oeteCENMZDA">Presentation video</a>
    </span>
</p>
<img src="/img/habitat/habitat-web.gif">
<hr>

<a name="/graph-parallel"></a>
<h2 class="pubt">Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations</h2>
<p class="pubd">
    <span class="authors">Anuroop Sriram, Abhishek Das, Brandon M. Wood, Siddharth Goyal, C. Lawrence Zitnick</span><br>
    <span class="conf">ICLR 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2203.09697">Paper</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/gemnet_gp">Code</a>
    </span>
</p>
<img src="/img/ocp/graph_parallel.png">
<hr>

<a name="/spinconv"></a>
<h2 class="pubt">Rotation Invariant Graph Neural Networks using Spin Convolutions</h2>
<p class="pubd">
    <span class="authors">Muhammed Shuaibi, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary Ulissi, C. Lawrence Zitnick</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2106.09575">Paper</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp">Code</a>
    </span>
</p>
<img src="/img/ocp/spinconv.jpg">
<hr>

<a name="/youdescribe-descriptions-1"></a>
<h2 class="pubt">Automated Video Description for Blind and Low Vision Users</h2>
<p class="pubd">
    <span class="authors">Aditya Bodi, Pooyan Fazli, Shasta Ihorn, Yue-Ting Siu, Andrew T Scott, Lothar Narins, Yash Kant, Abhishek Das, Ilmi Yoon</span><br>
    <span class="conf">CHI EA 2021</span><br>
    <span class="links">
        <a target="_blank" href="https://dl.acm.org/doi/10.1145/3411763.3451810">Paper</a>
    </span>
</p>
<img src="/img/youdescribe/chi_ea_system.png">
<hr>

<a name="/habitat-objnav"></a>
<h2 class="pubt">Auxiliary Tasks and Exploration Enable ObjectNav</h2>
<p class="pubd">
    <span class="authors">Joel Ye, Dhruv Batra, Abhishek Das, Erik Wijmans</span><br>
    <span class="conf">ICCV 2021</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2104.04112">Paper</a>
        <a target="_blank" href="https://github.com/joel99/objectnav">Code</a>
        <a target="_blank" href="https://joel99.github.io/objectnav/">Website</a>
    </span>
</p>
<img src="/img/habitat/habitat-objnav.png">
<hr>

<a name="/forcenet"></a>
<h2 class="pubt">ForceNet: A Graph Neural Network for Large-Scale Quantum Calculations</h2>
<p class="pubd">
    <span class="authors">Weihua Hu, Muhammed Shuaibi, Abhishek Das, Siddharth Goyal, Anuroop Sriram, Jure Leskovec, Devi Parikh, C. Lawrence Zitnick</span><br>
    <span class="conf">ICLR 2021 Deep Learning for Simulation Workshop</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2103.01436">Paper</a>
        <a target="_blank" href="https://opencatalystproject.org/">opencatalystproject.org</a>
        <a target="_blank" href="https://slideslive.com/38955314/forcenet-a-graph-neural-network-for-largescale-quantum-calculations?ref=speaker-22639-latest">Presentation video</a>
    </span>
</p>
<img src="/img/ocp/forcenet.jpg">
<hr>

<a name="/ocp-dataset"></a>
<h2 class="pubt">The Open Catalyst 2020 (OC20) Dataset and Community Challenges</h2>
<p class="pubd">
    <span class="authors">Lowik Chanussot<sup>*</sup>, Abhishek Das<sup>*</sup>, Siddharth Goyal<sup>*</sup>, Thibaut Lavril<sup>*</sup>, Muhammed Shuaibi<sup>*</sup>, Morgane Riviére, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, Zachary Ulissi</span><br>
    <span class="conf">ACS Catalysis 2021</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2010.09990">Paper</a>
        <a target="_blank" href="https://github.com/open-catalyst-project/ocp">Code</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/blob/main/DATASET.md">Dataset</a>
        <a target="_blank" href="https://opencatalystproject.org/">opencatalystproject.org</a>
    </span>
</p>
<img src="/img/ocp/dataset.png">
<hr>

<a name="/ocp-whitepaper"></a>
<h2 class="pubt">An Introduction to Electrocatalyst Design using Machine Learning for Renewable Energy Storage</h2>
<p class="pubd">
    <span class="authors">C. Lawrence Zitnick, Lowik Chanussot, Abhishek Das, Siddharth Goyal, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Thibaut Lavril, Aini Palizhati, Morgane Riviére, Muhammed Shuaibi, Anuroop Sriram, Kevin Tran, Brandon Wood, Junwoong Yoon, Devi Parikh, Zachary Ulissi</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2010.09435">Paper</a>
        <a target="_blank" href="https://opencatalystproject.org/">opencatalystproject.org</a>
    </span>
    <!-- Press: -->
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="https://ai.facebook.com/blog/facebook-and-carnegie-mellon-launch-the-open-catalyst-project-to-find-new-ways-to-store-renewable-energy">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/fair2.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Facebook and Carnegie Mellon launch .. to ... store renewable energy" by Larry Zitnick</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://fortune.com/2020/10/14/facebook-ai-open-catalyst-dataset-chemistry-renewable-energy/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/fortune.jpg" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Facebook A.I. researchers push for a breakthrough in renewable energy storage" by Jeremy Kahn</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://engadget.com/facebook-deploys-its-ai-to-find-green-energy-storage-solutions-130041147.html">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/engadget.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Facebook deploys its AI to find green energy storage solutions" by Andrew Tarantola</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.cnbc.com/2020/10/14/facebook-to-use-ai-in-bid-to-improve-renewable-energy-storage.html">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/cnbc.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Facebook to use artificial intelligence in bid to improve renewable energy storage" by Sam Shead</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://venturebeat.com/2020/10/14/facebook-and-carnegie-mellon-launch-project-to-discover-better-ways-to-store-renewable-energy/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/venturebeat.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Facebook and Carnegie Mellon launch project to ... store renewable energy" by Kyle Wiggers</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.cnet.com/news/facebook-plans-to-use-ai-to-help-fight-climate-change/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0 18px;">
                <img src="/img/logos/cnet.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Facebook plans to use AI to help fight climate change" by Queenie Wong</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://syncedreview.com/2020/10/15/facebook-cmu-open-catalyst-project-applies-ai-to-renewable-energy-storage/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/synced.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Facebook & CMU Open Catalyst Project Applies AI to Renewable Energy Storage" by Fangyu Cai</span>
            </div>
        </a>
    </div>
</p>
<video autoplay loop src="/img/ocp/relaxation.mp4" width="95%"></video>
<hr>

<a name="/habitat-pointnav-aux"></a>
<h2 class="pubt">Auxiliary Tasks Speed Up Learning PointGoal Navigation</h2>
<p class="pubd">
    <span class="authors">Joel Ye, Dhruv Batra, Erik Wijmans*, Abhishek Das*</span><br>
    <span class="conf">CoRL 2020</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2007.04561">Paper</a>
        <a target="_blank" href="https://github.com/joel99/habitat-pointnav-aux">Code</a>
    </span>
</p>
<img src="/img/habitat/habitat-pointnav-aux.jpg">
<hr>

<a name="/visdial-bert"></a>
<h2 class="pubt">Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline</h2>
<p class="pubd">
    <span class="authors">Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das</span><br>
    <span class="conf">ECCV 2020</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1912.02379">Paper</a>
        <a target="_blank" href="https://github.com/vmurahari3/visdial-bert">Code</a>
    </span>
</p>
<img src="/img/visdial/visdial-bert.jpg">
<hr>

<a name="/phd-thesis-1"></a>
<h2 class="pubt">Building agents that can see, talk, and act</h2>
<p class="pubd">
    <span class="authors">
        Abhishek Das
    </span><br>
    <span class="links">
        <a target="_blank" href="https://drive.google.com/file/u/2/d/1b2Gonazl1Os0eLPV9frkucEqSuRroEvD/view?usp=sharing">PhD Thesis</a>
    </span>
    <span class="conf">AAAI/ACM SIGAI Doctoral Dissertation Award, Runner-up</span><br>
    <span class="conf">Georgia Tech Sigma Xi Best PhD Thesis Award</span><br>
    <span class="conf">Georgia Tech College of Computing Dissertation Award</span>
</p>
<hr>

<a name="/qa-probing"></a>
<h2 class="pubt">Probing Emergent Semantics in Predictive Agents via Question Answering</h2>
<p class="pubd">
    <span class="authors">Abhishek Das<sup>*</sup>, Federico Carnevale<sup>*</sup>,
        Hamza Merzic, Laura Rimell, Rosalia Schneider, Josh Abramson, Alden Hung,
        Arun Ahuja, Stephen Clark, Gregory Wayne, Felix Hill
    </span><br>
    <span class="conf">ICML 2020</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2006.01016">Paper</a>
        <a target="_blank" href="https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering">Presentation video</a>
        <a target="_blank" href="https://docs.google.com/presentation/d/1yjfu2YBLTwJZXG4IiBiws4Z0EIts4X0J5DrYmYpBQ0A/edit?usp=sharing">Slides</a>
    </span>
</p>
<img src="/img/qa-probing/qa-probing-teaser.jpg">
<hr>

<a name="/dancing-agents"></a>
<h2 class="pubt">Feel The Music: Automatically Generating A Dance For An Input Song</h2>
<p class="pubd">
    <span class="authors">Purva Tendulkar, Abhishek Das, Aniruddha Kembhavi, Devi Parikh</span><br>
    <span class="conf">ICCC 2020</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2006.11905">Paper</a>
        <a target="_blank" href="https://github.com/purvaten/feel-the-music">Code</a>
        <a target="_blank" href="https://sites.google.com/view/dancing-agents">Videos</a>
    </span>
</p>
<img src="/img/dancing-agents/dancing-agents-teaser.jpg">
<hr>

<a name="/ds-vic"></a>
<h2 class="pubt">IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL</h2>
<p class="pubd">
    <span class="authors">Nirbhay Modhe, Prithvijit Chattopadhyay, Mohit Sharma, Abhishek Das, Devi Parikh, Dhruv Batra, Ramakrishna Vedantam</span><br>
    <span class="conf">IJCAI-PRICAI 2020, ICLR 2019 Task-Agnostic RL Workshop</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1907.10580">Paper</a>
    </span>
</p>
<img src="/img/ds-vic/teaser.jpg">
<hr>

<a name="/visdial-rl-plus-plus"></a>
<h2 class="pubt">Improving Generative Visual Dialog by Answering Diverse Questions</h2>
<p class="pubd">
    <span class="authors">Vishvak Murahari, Prithvijit Chattopadhyay, Dhruv Batra, Devi Parikh, Abhishek Das</span><br>
    <span class="conf">EMNLP 2019</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1909.10470">Paper</a>
        <a target="_blank" href="https://github.com/vmurahari3/visdial-diversity">Code</a>
    </span>
</p>
<img src="/img/visdial/visdial-rl-plus-plus.png">
<hr>

<a name="/multi-agent-comm"></a>
<h2 class="pubt">TarMAC: Targeted Multi-Agent Communication</h2>
<p class="pubd">
    <span class="authors">Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Michael Rabbat, Joelle Pineau</span><br>
    <span class="conf">ICML 2019</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1810.11187">Paper</a>
        <a target="_blank" href="https://drive.google.com/open?id=1ZjKogiYrqFVuBmad3IzkNW18pvP2QcIG">Slides</a>
    </span>
</p>
<img src="/img/multi-agent-comm/model.jpg">
<br><br>
<img src="/img/multi-agent-comm/shapes.gif">
<hr>

<a name="/eqa-mp3d"></a>
<h2 class="pubt">Embodied Question Answering in Photorealistic Environments with Point Clouds</h2>
<p class="pubd">
    <span class="authors">
        Erik Wijmans*, Samyak Datta*, Oleksandr Maksymets*, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra
    </span><br>
    <span class="conf">CVPR 2019 (Oral)</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1904.03461">Paper</a>
    </span>
</p>
<img src="/img/eqa/eqa-mp3d.png">
<hr>

<a name="/avsd"></a>
<h2 class="pubt">Audio-Visual Scene-Aware Dialog</h2>
<p class="pubd">
    <span class="authors">
        Huda Alamri, Vincent Cartillier, Abhishek Das,
        Jue Wang, Stefan Lee, Peter Anderson, Irfan Essa, Devi Parikh,
        Dhruv Batra, Anoop Cherian, Tim K. Marks, Chiori Hori
    </span><br>
    <span class="conf">CVPR 2019</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1901.09107">Paper</a>
        <a target="_blank" href="https://github.com/batra-mlp-lab/avsd">Code</a>
        <a target="_blank" href="http://video-dialog.com/">video-dialog.com</a>
    </span>
</p>
<img src="/img/avsd/avsd.jpg">
<hr>

<a name="/avsd_icassp"></a>
<h2 class="pubt">End-to-end Audio Visual Scene-Aware Dialog Using Multimodal Attention-based Video Features</h2>
<p class="pubd">
    <span class="authors">
            Chiori Hori, Huda Alamri, Jue Wang, Gordon Wichern, Takaaki Hori, Anoop Cherian, Tim K. Marks, Vincent Cartillier, Raphael Lopes, Abhishek Das, Irfan Essa, Dhruv Batra, Devi Parikh
    </span><br>
    <span class="conf">ICASSP 2019</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1806.08409">Paper</a>
        <a target="_blank" href="http://video-dialog.com/">video-dialog.com</a>
    </span>
</p>
<img src="/img/avsd/avsd_icassp.jpg">
<hr>

<a name="/eqa-modular"></a>
<h2 class="pubt">Neural Modular Control for Embodied Question Answering</h2>
<p class="pubd">
    <span class="authors">Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra</span><br>
    <span class="conf">CoRL 2018 (Spotlight)</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1810.11181">Paper</a>
        <a target="_blank" href="https://embodiedqa.org/">embodiedqa.org</a>
        <a target="_blank" href="https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330">Presentation video</a>
        <a target="_blank" href="https://drive.google.com/open?id=1xTvgpVNxG7MPZQe6jtXuYUIT2WPtoh0U">Slides</a>
    </span>
</p>

<img src="/img/eqa/eqa-modular.png">

<hr>
<a name="/embodied-qa"></a>
<h2 class="pubt">Embodied Question Answering</h2>
<p class="pubd">
    <span class="authors">Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra</span><br>
    <span class="conf">CVPR 2018 (Oral)</span><br>
    <span class="links">
        <a target="_blank" href="https://embodiedqa.org/paper.pdf">Paper</a>
        <a target="_blank" href="https://embodiedqa.org/">embodiedqa.org</a>
        <a target="_blank" href="https://github.com/facebookresearch/EmbodiedQA">Code</a>
        <a target="_blank" href="//youtu.be/gz2VoDrvX-A?t=1h19m58s">Presentation video</a>
        <a target="_blank" href="https://drive.google.com/open?id=1UacybW4p_8PDPNUvnEl05_89tbeG0ItP">Slides</a>
    </span>
    <!-- Press: -->
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="https://mlatgt.blog/2018/02/26/embodied-question-answering/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/mlgt.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Embodied Question Answering" by Abhishek Das</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://code.facebook.com/posts/1622140391226436/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/fair2.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"... a goal-driven approach to autonomous agents" by Dhruv Batra, Devi Parikh</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.technologyreview.com/s/611040/facebook-helped-create-an-ai-scavenger-hunt-that-could-lead-to-the-first-useful-home-robots/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 3px 0;">
                <img src="/img/logos/mittechreview.svg" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"... an AI scavenger hunt that could lead to the first useful home robots" by Will Knight</span>
            </div>
        </a>
    </div>
</p>

<img src="/img/eqa/teaser.jpg">

<hr>
<h2 class="pubt">Evaluating Visual Conversational Agents via Cooperative Human-AI Games</h2>
<p class="pubd">
    <span class="authors">Prithvijit Chattopadhyay<sup>*</sup>, Deshraj Yadav<sup>*</sup>, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh</span><br>
    <span class="conf">HCOMP 2017</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1708.05122">Paper</a>
        <a target="_blank" href="//github.com/VT-vision-lab/guesswhich">Code</a>
    </span>
</p>

<img src="/img/guesswhich/teaser.jpg">

<a name="/visdial-rl"></a>

<hr>
<h2 class="pubt">Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning</h2>
<p class="pubd">
    <span class="authors">Abhishek Das<sup>*</sup>, Satwik Kottur<sup>*</sup>, Stefan Lee, José M.F. Moura, Dhruv Batra</span><br>
    <span class="conf">ICCV 2017 (Oral)</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1703.06585">Paper</a>
        <a target="_blank" href="//github.com/batra-mlp-lab/visdial-rl">Code</a>
        <a target="_blank" href="//www.youtube.com/watch?v=R4hugGnNr7s">Presentation video</a>
        <a target="_blank" href="https://drive.google.com/open?id=0B70NAN5i4ZHSaVBESEFHQW9vUk0">Slides</a>
    </span>
</p>

<img src="/img/visdial/qbot_abot.jpg">

<hr>
<h2 class="pubt">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</h2>
<p class="pubd">
    <span class="authors">Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra</span><br>
    <span class="conf">IJCV 2019, ICCV 2017, NIPS 2016 Interpretable ML for Complex Systems Workshop</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1610.02391">Paper</a>
        <a target="_blank" href="https://github.com/ramprs/grad-cam">Code</a>
        <a target="_blank" href="http://gradcam.cloudcv.org/">Demo</a>
    </span>
</p>

<img src="/img/grad-cam/teaser.png">

<a name="/visdial"></a>

<hr>
<h2 class="pubt">Visual Dialog</h2>
<p class="pubd" style="margin-bottom:20px;">
    <span class="authors">Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M.F. Moura, Devi Parikh, Dhruv Batra</span><br>
    <span class="conf">PAMI 2018, CVPR 2017 (Spotlight)</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1611.08669">Paper</a>
        <a target="_blank" href="//github.com/batra-mlp-lab/visdial">Code</a>
        <a target="_blank" href="http://visualdialog.org/">visualdialog.org</a>
        <a target="_blank" href="https://github.com/batra-mlp-lab/visdial-amt-chat">AMT chat interface</a>
        <a target="_blank" href="http://demo.visualdialog.org">Demo</a>
        <a target="_blank" href="//www.youtube.com/watch?v=I9OlorMh7wU">Presentation video</a>
        <a target="_blank" href="https://drive.google.com/open?id=0B70NAN5i4ZHSTWhRTTlMdVVIcFU">Slides</a>
    </span>
</p>

<img src="/img/visdial/teaser.png">

<!-- <div id="vimeo-embed">
    <iframe src="https://player.vimeo.com/video/193092429?byline=0&portrait=0&color=ffffff" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div> -->

<hr>
<h2 class="pubt">Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?</h2>

<p class="pubd">
    <span class="authors">Abhishek Das<sup>*</sup>, Harsh Agrawal<sup>*</sup>, C. Lawrence Zitnick, Devi Parikh, Dhruv Batra</span> <br>
    <span class="conf">CVIU 2017, EMNLP 2016, ICML 2016 Workshop on Visualization for Deep Learning</span><br>
    <span class="links">
        <a target="_blank" href="//arxiv.org/abs/1606.03556">Paper</a>
        <a target="_blank" href="https://abhishekdas.com/vqa-hat/">Project+Dataset</a>
        <a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention</a>
    </span>
    <!-- Press: -->
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0; line-height: 1.1em;">
                <img src="/img/logos/nautilus.png" style="background: white; width: 57px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Is Artificial Intelligence Permanently Inscrutable?" by Aaron Bornstein</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="http://www.theverge.com/2016/7/12/12158238/first-click-deep-learning-algorithmic-black-boxes">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/theverge.png" style="margin-right: 5px; background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Deep learning is creating computer systems we don't fully understand" by James Vincent</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.newscientist.com/article/2095616-robot-eyes-and-humans-fix-on-different-things-to-decode-a-scene/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/newscientist.jpg" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Robot eyes and humans fix on different things to decode a scene" by Aviva Rutkin</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="http://www.techradar.com/news/world-of-tech/robots-and-humans-see-the-world-differently-but-we-don-t-know-why-1324165">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/techradar.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"Robots and humans see the world differently – but we don't know why" by Duncan Geere</span>
            </div>
        </a>
        <a style="border-bottom: 0;" target="_blank" href="https://www.technologyreview.com/s/601819/ai-is-learning-to-see-the-world-but-not-the-way-humans-do/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 3px 0;">
                <img src="/img/logos/mittechreview.svg" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"AI Is Learning to See the World—But Not the Way Humans Do" by Jamie Condliffe</span>
            </div>
        </a>
    </div>
</p><img src="/img/vqa-hat/teaser.jpg">
<hr>

<a name="/talks"></a>

# Talks

<div class="row">
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/visdial_rl_iccv17.jpg">
        </p>
    </div>
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/embodiedqa_cvpr18_4.jpg">
        </p>
    </div>
</div>
<div class="row">
    <div class="col-xs-12">
        <div class="talkt">
            <a target="_blank" href="https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering">
                ICML 2020: Probing Emergent Semantics in Predictive Agents via Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://slideslive.com/38917625/tarmac-targeted-multiagent-communication">
                ICML 2019 Imitation, Intent, and Interaction Workshop:
                Targeted Multi-Agent Communication
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.facebook.com/icml.imls/videos/444326646299556/">
                ICML 2019 Oral: Targeted Multi-Agent Communication
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=WxYBp3Xr_Nc">
                Allen Institute for Artificial Intelligence: "Towards Agents that can See, Talk, and Act"
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330">
                CoRL 2018 Spotlight: Neural Modular Control for Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/gz2VoDrvX-A?t=1h19m58s">
                CVPR 2018 Oral: Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://on-demand.gputechconf.com/gtc/2018/video/S8582/">
                NVIDIA GTC 2018
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=R4hugGnNr7s">
                ICCV 2017 Oral: Learning Cooperative Visual Dialog Agents with Deep RL
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/KAlGWMJnWyc?t=26m56s">
                Visual Question Answering Challenge Workshop, CVPR 2017
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=I9OlorMh7wU">
                CVPR 2017 Spotlight: Visual Dialog
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://techtalks.tv/talks/towards-transparent-visual-question-answering-systems/63026/">
                Visualization for Deep Learning Workshop, ICML 2016
            </a>
        </div>
    </div>
</div>
<hr>

<a name="/projects"></a>

# Side projects

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://aipaygrad.es">aipaygrad.es</a></h2>
        <p class="talkd">
            aipaygrad.es provides statistics of industry job offers in Artificial Intelligence (AI).
            All data is anonymous, cross-verified against offer letters and will
            hopefully reduce information asymmetry.
            <a target="_blank" href="http://aipaygrad.es"><img style="margin-top: 10px;" src="/img/projects/ai-paygrades.png"></a>
        </p>
    </div>
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://aideadlin.es">aideadlin.es</a></h2>
        <p class="talkd">
            aideadlin.es is a webpage to keep track of CV/NLP/ML/AI conference deadlines. It's hosted on GitHub, and countdowns are automatically updated via pull requests to the data file in the repo.
            <a target="_blank" href="http://aideadlin.es"><img style="margin-top: 10px;" src="/img/projects/ai-deadlines-1547012831.png"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention</a></h2>
        <p class="talkd">
            Torch implementation of an attention-based visual question answering model (Yang et al., CVPR16).
            The model looks at an image, reads a question, and comes up with an answer to the question and a heatmap of where it looked in the image to answer it.
            Some results <a href="https://computing.ece.vt.edu/~abhshkdz/neural-vqa-attention/figures/">here</a>.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention"><img class="project-img" src="/img/projects/neural-vqa-attention.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa">neural-vqa</a></h2>
        <p class="talkd">
            neural-vqa is an efficient, GPU-based Torch implementation of the visual question answering model from the NIPS 2015 paper 'Exploring Models and Data for Image Question Answering' by Ren et al.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa"><img src="/img/projects/neural-vqa.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://erdos.sdslabs.co">Erdős</a></h2>
        <p class="talkd">
            Erdős by <a target="_blank" href="//sdslabs.co">SDSLabs</a> is a competitive math learning platform, similar in spirit to <a href="https://projecteuler.net/">Project Euler</a>, albeit more feature-packed (support for holding competitions, has a social layer) and prettier.
            <a target="_blank" href="https://erdos.sdslabs.co"><img style="margin-top:10px;" src="/img/projects/erdos.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/graf">graf</a></h2>
        <p class="talkd">
            graf plots pretty git contribution bar graphs in the terminal.
            <code>gem install graf</code> to install.
            <a target="_blank" href="https://github.com/abhshkdz/graf"><img style="margin-top:10px;" src="/img/projects/graf.gif"></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/HackFlowy">HackFlowy</a></h2>
        <p class="talkd">
            Clone of <a href="//workflowy.com">WorkFlowy.com</a>, a beautiful, list-based note-taking website that has a 500-item monthly limit on the free tier :-(. This project is an open-source clone of WorkFlowy. "Make lists. Not war." :-)
            <a target="_blank" href="https://github.com/abhshkdz/HackFlowy"><img style="margin-top:40px;" src="/img/projects/hackflowy.png"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/AirMaps">AirMaps</a></h2>
        <p class="talkd">
            AirMaps was a fun hackathon project that lets users navigate through Google Earth with gestures and speech commands using a Kinect sensor. It was the <a target="_blank" href="https://blog.sdslabs.co/2014/02/code-fun-do">winning entry in Microsoft Code.Fun.Do</a>.
            <a target="_blank" href="https://github.com/abhshkdz/AirMaps"><img style="margin-top:10px;" src="/img/projects/airmaps.jpg"></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/sdslabs/hackview">HackView</a></h2>
        <p class="talkd">
            Another fun hackathon-winning project built during Yahoo! HackU! 2012 that involves webRTC-based P2P video chat, and was faster than any other video chat provider (at the time, before Google launched Hangouts).
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/8tracks-downloader">8tracks-downloader</a></h2>
        <p class="talkd">
            Ugly-looking, but super-effective bash script for downloading entire playlists from 8tracks. (Still works as of 10/2016).
        </p>
    </div>
</div>

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---

[1]: //mlp.cc.gatech.edu
[2]: ///www.cc.gatech.edu/~dbatra/
[3]: //www.cc.gatech.edu/~parikh/
[4]: //www.qbi.uq.edu.au/professor-geoffrey-goodhill
[5]: //researchers.uq.edu.au/researcher/2490
[6]: http://cns.qbi.uq.edu.au/
[7]: //developers.google.com/open-source/gsoc/
[8]: /posts/summer-of-code/
[9]: /posts/gsoc-reunion-2014/
[10]: //blog.sdslabs.co/2012/09/hacku
[11]: //blog.sdslabs.co/2014/02/code-fun-do
[12]: //www.facebook.com/SDSLabs/posts/527540147292475
[13]: /posts/deloitte-cctc-3/
[14]: /posts/google-india-community-summit/
[15]: //blog.sdslabs.co/2013/10/syntax-error-2013
[16]: //sdslabs.co/
[17]: //erdos.sdslabs.co/
[18]: //projecteuler.net/
[19]: //github.com/abhshkdz/neural-vqa
[20]: //github.com/abhshkdz/HackFlowy
[21]: //github.com/abhshkdz/graf
[22]: //github.com/abhshkdz
[23]: //twitter.com/abhshkdz
[24]: //instagram.com/abhshkdz
[25]: http://x.abhishekdas.com/
[26]: https://abhishekdas.com/vqa-hat/
[27]: http://arxiv.org/abs/1606.03556
[28]: https://www.newscientist.com/article/2095616-robot-eyes-and-humans-fix-on-different-things-to-decode-a-scene/
[29]: https://www.technologyreview.com/s/601819/ai-is-learning-to-see-the-world-but-not-the-way-humans-do/
[30]: http://www.theverge.com/2016/7/12/12158238/first-click-deep-learning-algorithmic-black-boxes
[31]: http://iitr.ac.in/
[32]: https://www.facebook.com/dhruv.batra.1253/posts/1783087161932290
[33]: https://drive.google.com/file/d/1nObeNzl-sTy8I5QN1Jv8wscebKLv-6RY/view?usp=sharing
[34]: http://aideadlin.es/
[35]: //github.com/abhshkdz/neural-vqa-attention
[36]: https://snapresearchfellowship.splashthat.com/
[37]: https://www.youtube.com/watch?v=R4hugGnNr7s
[38]: https://www.youtube.com/watch?v=I9OlorMh7wU
[39]: https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/
[40]: https://embodiedqa.org/
[41]: https://youtu.be/KAlGWMJnWyc?t=26m56s
[42]: https://2018gputechconf.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=152715
[43]: https://www.ic.gatech.edu/news/600684/three-ic-students-earn-snap-research-awards
[44]: https://www.ic.gatech.edu/news/601084/new-research-fellowships-offer-two-students-funding-access-adobes-creative-cloud
[45]: https://github.com/facebookresearch/House3D
[46]: https://gkioxari.github.io/
[47]: https://research.fb.com/people/parikh-devi/
[48]: https://research.fb.com/people/batra-dhruv/
[49]: https://lvatutorial.github.io/
[50]: http://acl2018.org/tutorials/#connecting-language-and-vis
[51]: http://visualqa.org/workshop.html
[52]: http://on-demand.gputechconf.com/gtc/2018/video/S8582/
[53]: https://visualdialog.org/challenge/2018
[54]: https://youtu.be/gz2VoDrvX-A?t=1h19m58s
[55]: https://research.fb.com/people/rabbat-mike/
[56]: https://www.cs.mcgill.ca/~jpineau/
[57]: https://visualdialog.org/challenge/2018#winners
[58]: https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330
[fb-fellow-page]: https://research.fb.com/announcing-the-2019-facebook-fellows-and-emerging-scholars/
[joelle-corl18-talk-mention]: https://www.youtube.com/watch?v=FSsEqEJKo8A&t=3497
[visdial-challenge-2]: https://visualdialog.org/challenge/2019
[ic-gt-article]: https://www.ic.gatech.edu/news/617061/see-and-say-abhishek-das-working-provide-crucial-communication-tools-intelligent-agents
[caliper]: https://caliper.ai
[felix-hill]: https://fh295.github.io
[laura-rimell]: http://www.rimell.cc/laura/
[stephen-clark]: https://sites.google.com/site/stephenclark609/
[andrej-karpathy]: https://karpathy.ai/
[vigil19]: https://vigilworkshop.github.io/2019
[tarmac-icml-talk]: https://www.facebook.com/icml.imls/videos/444326646299556/
[mastodon]: https://mastodon.social/web/accounts/1011404
[conquerearth]: https://conquer.earth/abhshkdz
[qa-probing-icml20-talk]: https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering
[vigil20]: https://vigilworkshop.github.io
[ocp]: https://opencatalystproject.org
[ocp-cnbc]: https://www.cnbc.com/2020/10/14/facebook-to-use-ai-in-bid-to-improve-renewable-energy-storage.html
[ocp-engadget]: https://engadget.com/facebook-deploys-its-ai-to-find-green-energy-storage-solutions-130041147.html
[ocp-fortune]: https://fortune.com/2020/10/14/facebook-ai-open-catalyst-dataset-chemistry-renewable-energy/
[ocp-venturebeat]: https://venturebeat.com/2020/10/14/facebook-and-carnegie-mellon-launch-project-to-discover-better-ways-to-store-renewable-energy/
[aipaygrad.es]: https://aipaygrad.es
[sigma-xi-thesis-award]: https://cpb-us-w2.wpmucdn.com/sites.gatech.edu/dist/0/283/files/2021/03/2021-Sigma-Xi-Research-Award-Winners.final_.pdf
[coc-dissertation-award]: https://sites.gatech.edu/gtcomputingawards2021/graduate-student-awards/
[thesis-pdf]: https://drive.google.com/file/u/2/d/1b2Gonazl1Os0eLPV9frkucEqSuRroEvD/view?usp=sharing
[aaai-dissertation-award]: https://aaai.org/Awards/dissertation-award.php
